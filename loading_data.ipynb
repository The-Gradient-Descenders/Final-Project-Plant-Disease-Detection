{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "import pathlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplake\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening dataset in read-only mode as you don't have write permissions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/plantvillage-without-augmentation\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\\"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://activeloop/plantvillage-without-augmentation loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " "
     ]
    }
   ],
   "source": [
    "# Loads plant_village dataset\n",
    "ds = deeplake.load('hub://activeloop/plantvillage-without-augmentation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 9.67 GiB for an array with shape (52803, 256, 256, 3) and data type uint8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Prints shape of plant_village images in np.ndarray format\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# This cell takes five minutes to run (rip)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m img \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39;49mimages\u001b[39m.\u001b[39;49mnumpy()\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39mshape(img))\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(img))\n",
      "File \u001b[1;32mc:\\Users\\mail2\\anaconda3\\envs\\bwsi_final\\Lib\\site-packages\\deeplake\\core\\tensor.py:788\u001b[0m, in \u001b[0;36mTensor.numpy\u001b[1;34m(self, aslist, fetch_chunks)\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnumpy\u001b[39m(\n\u001b[0;32m    764\u001b[0m     \u001b[39mself\u001b[39m, aslist\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, fetch_chunks\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    765\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[np\u001b[39m.\u001b[39mndarray, List[np\u001b[39m.\u001b[39mndarray]]:\n\u001b[0;32m    766\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Computes the contents of the tensor in numpy format.\u001b[39;00m\n\u001b[0;32m    767\u001b[0m \n\u001b[0;32m    768\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[39m        For tensors of htype ``polygon``, aslist is always ``True``.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 788\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_engine\u001b[39m.\u001b[39;49mnumpy(\n\u001b[0;32m    789\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex,\n\u001b[0;32m    790\u001b[0m         aslist\u001b[39m=\u001b[39;49maslist,\n\u001b[0;32m    791\u001b[0m         fetch_chunks\u001b[39m=\u001b[39;49mfetch_chunks \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mis_iteration,\n\u001b[0;32m    792\u001b[0m         pad_tensor\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad_tensor,\n\u001b[0;32m    793\u001b[0m     )\n\u001b[0;32m    794\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpoint_cloud\u001b[39m\u001b[39m\"\u001b[39m:  \u001b[39m# TODO: refactor\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(ret, \u001b[39mlist\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\mail2\\anaconda3\\envs\\bwsi_final\\Lib\\site-packages\\deeplake\\core\\chunk_engine.py:1797\u001b[0m, in \u001b[0;36mChunkEngine.numpy\u001b[1;34m(self, index, aslist, use_data_cache, fetch_chunks, pad_tensor)\u001b[0m\n\u001b[0;32m   1795\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_link_ready()\n\u001b[0;32m   1796\u001b[0m fetch_chunks \u001b[39m=\u001b[39m fetch_chunks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_full_chunk(index)\n\u001b[1;32m-> 1797\u001b[0m \u001b[39mreturn\u001b[39;00m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sequence_numpy \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mis_sequence \u001b[39melse\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy)(\n\u001b[0;32m   1798\u001b[0m     index, aslist, use_data_cache, fetch_chunks, pad_tensor\n\u001b[0;32m   1799\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\mail2\\anaconda3\\envs\\bwsi_final\\Lib\\site-packages\\deeplake\\core\\chunk_engine.py:2026\u001b[0m, in \u001b[0;36mChunkEngine._numpy\u001b[1;34m(self, index, aslist, use_data_cache, fetch_chunks, pad_tensor)\u001b[0m\n\u001b[0;32m   2024\u001b[0m \u001b[39mif\u001b[39;00m aslist:\n\u001b[0;32m   2025\u001b[0m     \u001b[39mreturn\u001b[39;00m samples\n\u001b[1;32m-> 2026\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(samples)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 9.67 GiB for an array with shape (52803, 256, 256, 3) and data type uint8"
     ]
    }
   ],
   "source": [
    "# Prints shape of plant_village images after turning it into np.ndarray format\n",
    "# This cell takes five minutes to run (rip)\n",
    "img = ds.images.numpy()\n",
    "print(np.shape(img))\n",
    "print(type(img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52803, 1)\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "# Prints shape of plant_village labels\n",
    "label = ds.labels.numpy(aslist=True)\n",
    "labels = np.array(label)\n",
    "\n",
    "print(labels.shape)\n",
    "print(labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'PlantDoc-Dataset'...\n",
      "error: invalid path 'test/Bell_pepper leaf/IMG_1629.JPG?1507122477.jpg'\n",
      "fatal: unable to checkout working tree\n",
      "warning: Clone succeeded, but checkout failed.\n",
      "You can inspect what was checked out with 'git status'\n",
      "and retry with 'git restore --source=HEAD :/'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clones PlantDoc dataset from GitHub into Google Colab\n",
    "# This cell could take a few minutes\n",
    "# print(os.getcwd())\n",
    "!git clone https://github.com/pratikkayal/PlantDoc-Dataset.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any of the parent directories): .git\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 files belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 0 files for training.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No images found in directory PlantDoc-Dataset. Allowed formats: ('.bmp', '.gif', '.jpeg', '.jpg', '.png')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m img_height \u001b[39m=\u001b[39m \u001b[39m256\u001b[39m\n\u001b[0;32m      9\u001b[0m img_width \u001b[39m=\u001b[39m \u001b[39m256\u001b[39m\n\u001b[1;32m---> 11\u001b[0m train_ds \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mimage_dataset_from_directory(\n\u001b[0;32m     12\u001b[0m   data_dir,\n\u001b[0;32m     13\u001b[0m   validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,\n\u001b[0;32m     14\u001b[0m   subset\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtraining\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     15\u001b[0m   seed\u001b[39m=\u001b[39;49m\u001b[39m123\u001b[39;49m,\n\u001b[0;32m     16\u001b[0m   image_size\u001b[39m=\u001b[39;49m(img_height, img_width),\n\u001b[0;32m     17\u001b[0m   batch_size\u001b[39m=\u001b[39;49mbatch_size)\n\u001b[0;32m     19\u001b[0m \u001b[39m# class_names = train_ds.class_names\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m# print(class_names)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \n\u001b[0;32m     22\u001b[0m \u001b[39m# Access PlantDoc testing dataset\u001b[39;00m\n\u001b[0;32m     23\u001b[0m data_dir \u001b[39m=\u001b[39m pathlib\u001b[39m.\u001b[39mPath(\u001b[39m\"\u001b[39m\u001b[39mPlantDoc-Dataset/test\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mwith_suffix(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mail2\\anaconda3\\envs\\bwsi_final\\Lib\\site-packages\\keras\\src\\utils\\image_dataset.py:299\u001b[0m, in \u001b[0;36mimage_dataset_from_directory\u001b[1;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[0;32m    295\u001b[0m image_paths, labels \u001b[39m=\u001b[39m dataset_utils\u001b[39m.\u001b[39mget_training_or_validation_split(\n\u001b[0;32m    296\u001b[0m     image_paths, labels, validation_split, subset\n\u001b[0;32m    297\u001b[0m )\n\u001b[0;32m    298\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m image_paths:\n\u001b[1;32m--> 299\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    300\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo images found in directory \u001b[39m\u001b[39m{\u001b[39;00mdirectory\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    301\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAllowed formats: \u001b[39m\u001b[39m{\u001b[39;00mALLOWLIST_FORMATS\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m     )\n\u001b[0;32m    304\u001b[0m dataset \u001b[39m=\u001b[39m paths_and_labels_to_dataset(\n\u001b[0;32m    305\u001b[0m     image_paths\u001b[39m=\u001b[39mimage_paths,\n\u001b[0;32m    306\u001b[0m     image_size\u001b[39m=\u001b[39mimage_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    312\u001b[0m     crop_to_aspect_ratio\u001b[39m=\u001b[39mcrop_to_aspect_ratio,\n\u001b[0;32m    313\u001b[0m )\n\u001b[0;32m    314\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mprefetch(tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mAUTOTUNE)\n",
      "\u001b[1;31mValueError\u001b[0m: No images found in directory PlantDoc-Dataset. Allowed formats: ('.bmp', '.gif', '.jpeg', '.jpg', '.png')"
     ]
    }
   ],
   "source": [
    "# Access PlantDoc training dataset\n",
    "# print(os.getcwd())\n",
    "data_dir = pathlib.Path(\"PlantDoc-Dataset/train\").with_suffix(\"\")\n",
    "\n",
    "image_count_train = len(list(data_dir.glob('*/*.jpg')))\n",
    "\n",
    "batch_size = 32\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "# class_names = train_ds.class_names\n",
    "# print(class_names)\n",
    "\n",
    "# Access PlantDoc testing dataset\n",
    "data_dir = pathlib.Path(\"PlantDoc-Dataset/test\").with_suffix(\"\")\n",
    "image_count_test = len(list(data_dir.glob('*/*.jpg')))\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into np.ndarray format\n",
    "\n",
    "plantDoc_images = np.ndarray([1, 256, 256, 3])\n",
    "plantDoc_labels = np.ndarray([1])\n",
    "plantDoc_images_test = np.ndarray([1, 256, 256, 3])\n",
    "plantDoc_labels_test = np.ndarray([1])\n",
    "\n",
    "count = 0\n",
    "for image_batch, labels_batch in train_ds:\n",
    "#   if count % 5 == 0:\n",
    "#     print(\"Batch number\", count)\n",
    "  plantDoc_images = np.append(plantDoc_images, image_batch.numpy(), axis = 0)\n",
    "  plantDoc_labels = np.append(plantDoc_labels, labels_batch.numpy(), axis = 0)\n",
    "  count += 1\n",
    "\n",
    "print(plantDoc_images.shape, plantDoc_labels.shape)\n",
    "\n",
    "count = 0\n",
    "for image_batch, labels_batch in test_ds:\n",
    "#   if count % 5 == 0:\n",
    "#     print(\"Batch number\", count)\n",
    "  plantDoc_images_test = np.append(plantDoc_images_test, image_batch.numpy(), axis = 0)\n",
    "  plantDoc_labels_test = np.append(plantDoc_labels_test, labels_batch.numpy(), axis = 0)\n",
    "  count += 1\n",
    "\n",
    "print(plantDoc_images_test.shape, plantDoc_labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "np.random.shuffle(img)\n",
    "np.random.shuffle(labels)\n",
    "\n",
    "np.random.shuffle(plantDoc_images)\n",
    "np.random.shuffle(plantDoc_labels)\n",
    "np.random.shuffle(plantDoc_images_test)\n",
    "np.random.shuffle(plantDoc_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate datasets\n",
    "train_images = np.append(img[0:40727], plantDoc_images, axis = 0)\n",
    "test_images = np.append(img[40728:52803], plantDoc_images_test, axis = 0)\n",
    "\n",
    "train_labels = np.append(labels[0:40727], plantDoc_labels, axis = 0)\n",
    "test_labels = np.append(labels[40728:52803], plantDoc_labels_test, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to files\n",
    "with open(\"train_images.pkl\", mode=\"wb\") as opened_file:\n",
    "    pickle.dump(train_images, opened_file)\n",
    "with open(\"test_images.pkl\", mode=\"wb\") as opened_file:\n",
    "    pickle.dump(test_images, opened_file)\n",
    "with open(\"train_labels.pkl\", mode=\"wb\") as opened_file:\n",
    "    pickle.dump(train_labels, opened_file)\n",
    "with open(\"test_labels.pkl\", mode=\"wb\") as opened_file:\n",
    "    pickle.dump(test_labels, opened_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bwsi_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
